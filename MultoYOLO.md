# Multiple stream, multiple GPU YOLO
So you have successfully mastered TrainYourOwnYOLO, and it happily processes any video stream you throw at it. But what about two, or more streams at the same time? No matter how big and powerful your GPU is, no matter how many GPUs you stick into your computer, YOLO just won’t let you process two, or more streams at once. 
Until now, that is. 
With this tweak to the popular TrainYourOwnYOLO, and with a halfway powerful GPU, you will be able to process video streams in the double digits. With the Tensorflow 1.x version of TrainYourOwnYOLO , I could wring 13 streams out of a by now antique Geforce 1080 ti. Tensorflow 2.3 used in the new version appears to use a little more memory. Now I can (barely) fit 11 streams into the GPU. The limiting factor is memory (both GPU and system, more of the latter below). The more memory on your GPU, the more streams it can accommodate. I am saving for the upcoming RTX 3090 with 24 G of memory, and it should be good for ~ 25 streams at the same time on the same GPU.  
## The problem.
What kept you from processing two or more streams at once is the fact that when you initialize the YOLO object, it allocates all available GPU memory for that instance. If there are more CUDA-enabled GPUs in the machine, the most powerful appears to be grabbed, and all others will be ignored. Try initializing a second Yolo instance, and you will get an “Unexpected CUDA error: out of memory,” never mind that there could be a second, complete idle GPU in your machine. 
There is a gpu_num flag in init_yolo, but it won’t let you assign a GPU. It purports to use multiple GPUs for inference. My tests showed that in the best case, that flag will get you a couple of fps more, in the worst case, yolo will slow down. I could not find anything in the code that allowed for separate YOLO instances, whether it’s on the same, or on multiple GPUs in one machine.  
## The solution.
Following init_yolo() down the dark rabbit-hole called of keras_yolo, and digging around its scant documentation, if developed an idea of what happens when you initialize the session. Left alone, the session indeed grabs all of the memory of the most powerful GPU in the machine, whether you need it, or not. However, there are other options Yolo currently does not use. 
config.gpu_options.per_process_gpu_memory_fraction  is the most important option of all. It allows to allocate just a fraction of the GPU memory. Set it to 0.5, and only half of the GPU memory will be used. (Close, but not quite, as we will see below.) The remainder of the memory will be available for subsequent sessions.
config.gpu_options.visible_device_list  allows to (hooray) select the GPU the session will use (assuming that you have more than one.) If set to “0,” the first GPU will be used, set to “1,” the second will be used, and so forth. “0,1” supposedly both GPUs to be used at the same time.  When I tried that setting, I received an “Unexpected CUDA error: out of memory” when the second process was fired-up, and I left that alone. A more knowledgeable and courageous soul possible will find out  more.
config.gpu_options.allow_growth allows you to use the memory on your GPU more sensibly:  Set to False, the session will grab all of the GPU’s available memory (or the assigned fraction thereof) whether it is needed, or not. Set to True, the session will claim only the memory it needs at initialization, and it will “grow” more memory if it needs it, as long as there is memory available. Run a single process with the allow_growth flag set to True. Watch GPU memory for a while.  If it doesn’t grow, then that’s all the GPU memory the process will ever need, more will simply go to waste. Under Tensorflow 1.X, a single stream with per_process_gpu_memory_fraction  set to 1, and with allow_growth enabled, claimed 8,107 MiB of the 11,178 MiB available on the GPU. With Tensorflow 2.3, the footprint grew to 8,575 MiB. I usually keep allow_growth set to true.
With these options in place, multiple Python processes can claim their own yolo instance and run in parallel. They can use the same model, or different ones. Of course, throughput per process will drop as you add more processes, after all, you are sharing GPU power. You can get around this problem by sending only every n-th frame to Yolo while displaying all. Depending on your use case, it probably won’t make much difference whether you try detecting an object twenty times, or four times a second.
##Improvements 
While we are at it, let’s implement a few improvements.
With the hush flag set, most if not all the annoying notices cluttering your monitor during init_yolo will be silenced.
Ignore_labels is a list of objects that will be ignored, and not reported by YOLO if detected. Use it to ignore objects you are no longer interested in. If you have problematic parts of your video stream that get mis-identified, you can train them as “NULL” or somesuch.  Ignore_labels=[“NULL”] will make it disappear. Ignore_labels takes a list, so you can shun multiple objects.
The strategically important routine is detect_image(). We feed it an image, and if a match (or matches) are found, the coordinates of the object and the confidence, along with an index into an object list are returned in  out_prediction, the image, adorned with bounding boxes, are returned in image.
What if we want the actual name of the  detected object, along with the time spent on the detection?  detect_image_extended() will provide that additional info. detect_image() is kept for backward compatibility.
(Tangent: detect_image spends quite some time drawing boxes, which made the original author leave an exasperated comment: “My kingdom for a good redistributable image drawing library.” OpenCV has a snappy box draw routine, and it is generally faster than the Pillow library used in detect_image. To shave off a few cycles, OpenCV could be used. If all you are interested in are the coordinates, and the detected object, the image drawing could be (optionally) avoided altogether. Room for improvement ….)
Other than GPU factors
Each separate process commands a good deal of memory, due to the fact that it creates its own YOLO object along with a completely separate model. On my system, TOP reports the resident memory footprint of one YOLO process as 2.8 Gigabytes. 10 streams amount to 28 Gigabytes, and many machines don’t have that much. While the GPU and memory plays the biggest role, the CPU is not that much of a factor. An ageing 4core Intel  6700K, taxed with 9 processes run in parallel on a 1080 ti, would deliver around 4 fps per process, while the load average zooming to 13. A beefy 32core monster, the Threadripper 3970x, also delivered 4 fps per each of the 9 processes, but with a load average of 3.8, it barely broke a sweat.  
##Needless to say, but said anyway: 
All the settings used in init_yolo are on a per-session basis. They can be completely different, or all the same from session to session. 
##Infamous last words
Development was on Ubuntu 18.04, with Python3.7. No other systems were tested. 
My programming skills are completely self-taught. I tried my hands on assembler and BASIC half a century ago, and I took up Python to keep me busy after retirement. My code definitely is in need of improvement, and it could be completely flawed. Have at it. 
